{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c2bbd4a",
   "metadata": {},
   "source": [
    "#Milestone 2.2 - Hash generation\n",
    "\n",
    "Steps done here:\n",
    "\n",
    "1. Compute spectrogram (STFT magnitude)\n",
    "2. Compute constellation map (peak picking)\n",
    "3. Extract peaks (time, frequency, magnitude)\n",
    "4. Generate **anchor–target hashes** inside configurable **target zones**\n",
    "5. Store hashes in an **inverted index**: `hash -> [(track_id, time_offset), ...]`\n",
    "6. Save the database and a small report (hash counts + file sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "8fed8806",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T14:03:59.737737Z",
     "start_time": "2026-01-16T14:03:59.703566Z"
    }
   },
   "source": [
    "# Imports\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from scipy import ndimage\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "9b97407d",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Edit `BASE_PATH`, `DB_AUDIO_PATH`, and `OUTPUT_*` to match your machine.\n",
    "\n",
    "- `DB_AUDIO_PATH` should point to the folder containing the database tracks.\n",
    "- The code supports `.mp3`, `.wav`, `.m4a`.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "eab160cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T14:04:01.951481Z",
     "start_time": "2026-01-16T14:04:01.944458Z"
    }
   },
   "source": [
    "# Paths\n",
    "BASE_PATH = r\"C:/Development/MusicIR\"\n",
    "DB_AUDIO_PATH = os.path.join(BASE_PATH, \"raw_30s_audio-26\")\n",
    "\n",
    "# Output folder for Task 1\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"task1_hash_db\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "OUTPUT_DB_FILE = os.path.join(OUTPUT_DIR, \"hash_db.pkl\")\n",
    "OUTPUT_REPORT_CSV = os.path.join(OUTPUT_DIR, \"hash_db_report.csv\")\n",
    "\n",
    "print(\"DB_AUDIO_PATH:\", DB_AUDIO_PATH)\n",
    "print(\"OUTPUT_DB_FILE:\", OUTPUT_DB_FILE)\n",
    "print(\"OUTPUT_REPORT_CSV:\", OUTPUT_REPORT_CSV)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB_AUDIO_PATH: C:/Development/MusicIR\\raw_30s_audio-26\n",
      "OUTPUT_DB_FILE: C:/Development/MusicIR\\task1_hash_db\\hash_db.pkl\n",
      "OUTPUT_REPORT_CSV: C:/Development/MusicIR\\task1_hash_db\\hash_db_report.csv\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "813ec18d",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "- **Peak picking**: we reuse the best-performing configuration from MS1: `dist_freq=12`, `dist_time=6`, `thresh=0.01`.\n",
    "- **Target zones**: four configurations using two frequency ranges × two time ranges (in frames).\n",
    "\n",
    "> Note: time windows are expressed in **frames**, because hashes are based on the time-frame index from the STFT.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "193b8753",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T14:04:03.685307Z",
     "start_time": "2026-01-16T14:04:03.666269Z"
    }
   },
   "source": [
    "# --- STFT parameters ---\n",
    "Fs = 22050\n",
    "N = 2048\n",
    "H = 1024\n",
    "\n",
    "# --- Peak picking parameters ---\n",
    "PEAK_DIST_FREQ = 12\n",
    "PEAK_DIST_TIME = 6\n",
    "PEAK_THRESH = 0.01\n",
    "\n",
    "# restrict bins to fit into 10-bit frequency (0..1023) cleanly\n",
    "# N=2048 produces 1025 bins (0..1024). Setting BIN_MAX=1024 drops the Nyquist bin.\n",
    "BIN_MAX = 1024\n",
    "\n",
    "# --- Target zones (two freq ranges × two time ranges) ---\n",
    "TARGET_ZONES = [\n",
    "    {\"name\": \"Fsmall_Tshort\", \"df\": 50,  \"dt_min\": 1, \"dt_max\": 20},\n",
    "    {\"name\": \"Fsmall_Tlong\",  \"df\": 50,  \"dt_min\": 1, \"dt_max\": 60},\n",
    "    {\"name\": \"Flarge_Tshort\", \"df\": 150, \"dt_min\": 1, \"dt_max\": 20},\n",
    "    {\"name\": \"Flarge_Tlong\",  \"df\": 150, \"dt_min\": 1, \"dt_max\": 60},\n",
    "]\n",
    "\n",
    "# limits number of target peaks per anchor (makes index size smaller)\n",
    "TOP_K_TARGETS = 5  # can be set to None to disable\n",
    "\n",
    "# Duration handling:\n",
    "# - Use None for full track (needed in MS2.2 Task 3 scaling)\n",
    "LOAD_DURATION = 30.0\n",
    "\n",
    "sec_per_frame = H / Fs\n",
    "print(f\"sec_per_frame = {sec_per_frame:.5f} s\")\n",
    "print(\"Target zones:\")\n",
    "for z in TARGET_ZONES:\n",
    "    print(\" \", z, f\"(~dt_max {z['dt_max']*sec_per_frame:.2f}s)\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sec_per_frame = 0.04644 s\n",
      "Target zones:\n",
      "  {'name': 'Fsmall_Tshort', 'df': 50, 'dt_min': 1, 'dt_max': 20} (~dt_max 0.93s)\n",
      "  {'name': 'Fsmall_Tlong', 'df': 50, 'dt_min': 1, 'dt_max': 60} (~dt_max 2.79s)\n",
      "  {'name': 'Flarge_Tshort', 'df': 150, 'dt_min': 1, 'dt_max': 20} (~dt_max 0.93s)\n",
      "  {'name': 'Flarge_Tlong', 'df': 150, 'dt_min': 1, 'dt_max': 60} (~dt_max 2.79s)\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "598d3dbe",
   "metadata": {},
   "source": [
    "## Core functions (spectrogram + constellation map)\n",
    "\n",
    "This is your MS1 implementation with two important fixes:\n",
    "\n",
    "1. `frame_max` now defaults to the **time** dimension (`X.shape[1]`), not the freq dimension.\n",
    "2. `duration` is parameterized via `LOAD_DURATION`.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "d92f73a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T14:04:05.532358Z",
     "start_time": "2026-01-16T14:04:05.515796Z"
    }
   },
   "source": [
    "def compute_spectrogram(fn_wav, Fs=22050, N=2048, H=1024, duration=30.0, bin_max=None, frame_max=None):\n",
    "    \"\"\"Computes the magnitude spectrogram of an audio file.\n",
    "\n",
    "    Args:\n",
    "        fn_wav (str): Path to the audio file\n",
    "        Fs (int): Sampling rate in Hz\n",
    "        N (int): FFT/window size\n",
    "        H (int): Hop size\n",
    "        duration (float|None): Seconds to load; None loads full audio\n",
    "        bin_max (int|None): Max frequency bin (exclusive)\n",
    "        frame_max (int|None): Max time frame (exclusive)\n",
    "\n",
    "    Returns:\n",
    "        Y (np.ndarray): Magnitude spectrogram (freq x time)\n",
    "    \"\"\"\n",
    "    x, Fs = librosa.load(fn_wav, sr=Fs, duration=duration)\n",
    "\n",
    "    X = librosa.stft(\n",
    "        x,\n",
    "        n_fft=N,\n",
    "        hop_length=H,\n",
    "        win_length=N,\n",
    "        window='hann'\n",
    "    )\n",
    "\n",
    "    if bin_max is None:\n",
    "        bin_max = X.shape[0]\n",
    "    if frame_max is None:\n",
    "        frame_max = X.shape[1]  # FIX: time dimension\n",
    "\n",
    "    Y = np.abs(X[:bin_max, :frame_max])\n",
    "    return Y\n",
    "\n",
    "\n",
    "def compute_constellation_map(Y, dist_freq=7, dist_time=7, thresh=0.01):\n",
    "    \"\"\"Compute constellation map via max-filter peak picking.\n",
    "\n",
    "    Args:\n",
    "        Y (np.ndarray): Magnitude spectrogram\n",
    "        dist_freq (int): neighborhood in freq\n",
    "        dist_time (int): neighborhood in time\n",
    "        thresh (float): min magnitude threshold\n",
    "\n",
    "    Returns:\n",
    "        Cmap (np.ndarray): boolean mask of peaks (same shape as Y)\n",
    "    \"\"\"\n",
    "    result = ndimage.maximum_filter(\n",
    "        Y,\n",
    "        size=[2 * dist_freq + 1, 2 * dist_time + 1],\n",
    "        mode='constant'\n",
    "    )\n",
    "    Cmap = np.logical_and(Y == result, result > thresh)\n",
    "    return Cmap\n"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "fbfa9538",
   "metadata": {},
   "source": [
    "## Peak extraction\n",
    "\n",
    "Convert the boolean constellation map to a **time-sorted** list of peaks:\n",
    "\n",
    "`(t_frame, f_bin, magnitude)`\n",
    "\n",
    "Time-sorting is essential so we can stop scanning targets once we pass `dt_max`.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "1547e71e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T14:04:08.056688Z",
     "start_time": "2026-01-16T14:04:08.038599Z"
    }
   },
   "source": [
    "def extract_peaks(Y, Cmap):\n",
    "    \"\"\"Extract peaks from constellation map as a list of (t, f, mag), sorted by t.\n",
    "\n",
    "    Args:\n",
    "        Y (np.ndarray): magnitude spectrogram (freq x time)\n",
    "        Cmap (np.ndarray): boolean peak mask\n",
    "\n",
    "    Returns:\n",
    "        peaks (list[tuple[int,int,float]]): (t, f, mag) sorted by t\n",
    "    \"\"\"\n",
    "    f_idx, t_idx = np.where(Cmap)\n",
    "    mags = Y[f_idx, t_idx]\n",
    "    peaks = list(zip(t_idx.tolist(), f_idx.tolist(), mags.tolist()))\n",
    "    peaks.sort(key=lambda x: x[0])\n",
    "    return peaks\n"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "971c7d91",
   "metadata": {},
   "source": [
    "## 32-bit hash packing\n",
    "\n",
    "We pack `(f_anchor, f_target, Δt)` into a 32-bit unsigned integer:\n",
    "\n",
    "- 10 bits: f_anchor (0..1023)\n",
    "- 10 bits: f_target (0..1023)\n",
    "- 12 bits: dt (0..4095)\n",
    "\n",
    "This fits the assignment requirement of a **32-bit hash**.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "8dc3cbb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T14:04:13.976449Z",
     "start_time": "2026-01-16T14:04:13.955884Z"
    }
   },
   "source": [
    "BITS_F = 10\n",
    "BITS_DT = 12\n",
    "MAX_F = (1 << BITS_F) - 1       # 1023\n",
    "MAX_DT = (1 << BITS_DT) - 1     # 4095\n",
    "\n",
    "\n",
    "def make_hash(f1, f2, dt):\n",
    "    \"\"\"Create a 32-bit hash for (f1, f2, dt).\n",
    "\n",
    "    Returns None if dt is out of range (shouldn't happen with our dt_max).\n",
    "    \"\"\"\n",
    "    # enforce representable ranges\n",
    "    if f1 > MAX_F: f1 = MAX_F\n",
    "    if f2 > MAX_F: f2 = MAX_F\n",
    "    if dt < 0 or dt > MAX_DT:\n",
    "        return None\n",
    "\n",
    "    # (f1 << 22) | (f2 << 12) | dt\n",
    "    return (int(f1) << (BITS_F + BITS_DT)) | (int(f2) << BITS_DT) | int(dt)\n",
    "\n",
    "\n",
    "def pack_posting(track_id, t_anchor):\n",
    "    \"\"\"Pack (track_id, t_anchor) into one 32-bit integer (16 bits each).\n",
    "\n",
    "    If you have more than 65535 tracks or time frames, switch to 64-bit packing.\n",
    "    \"\"\"\n",
    "    return (int(track_id) << 16) | (int(t_anchor) & 0xFFFF)\n",
    "\n",
    "\n",
    "def unpack_posting(p):\n",
    "    track_id = int(p) >> 16\n",
    "    t_anchor = int(p) & 0xFFFF\n",
    "    return track_id, t_anchor\n"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "d86ffd46",
   "metadata": {},
   "source": [
    "## Hash generation (anchor → targets inside target zone)\n",
    "\n",
    "For each anchor peak, we consider peaks in the time window `[dt_min, dt_max]` and within a frequency band `±df`.\n",
    "\n",
    "To keep the index size manageable, we optionally keep only the **TOP_K_TARGETS** strongest targets per anchor.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "c3c353d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T14:04:16.532011Z",
     "start_time": "2026-01-16T14:04:16.520498Z"
    }
   },
   "source": [
    "def generate_hashes_from_peaks(peaks, zone, top_k=None):\n",
    "    \"\"\"Generate (hash, anchor_time) pairs for one track and one target zone.\n",
    "\n",
    "    Args:\n",
    "        peaks: list of (t, f, mag) sorted by t\n",
    "        zone: dict with keys {df, dt_min, dt_max}\n",
    "        top_k: keep only K strongest targets per anchor (by target mag). None disables.\n",
    "\n",
    "    Returns:\n",
    "        pairs: list of (hash_value, t_anchor)\n",
    "    \"\"\"\n",
    "    df = zone[\"df\"]\n",
    "    dt_min = zone[\"dt_min\"]\n",
    "    dt_max = zone[\"dt_max\"]\n",
    "\n",
    "    out = []\n",
    "    n = len(peaks)\n",
    "\n",
    "    for i in range(n):\n",
    "        t1, f1, m1 = peaks[i]\n",
    "\n",
    "        candidates = []\n",
    "        for j in range(i + 1, n):\n",
    "            t2, f2, m2 = peaks[j]\n",
    "            dt = t2 - t1\n",
    "\n",
    "            if dt < dt_min:\n",
    "                continue\n",
    "            if dt > dt_max:\n",
    "                break  # peaks are time-sorted\n",
    "\n",
    "            if abs(f2 - f1) <= df:\n",
    "                candidates.append((f2, dt, m2))\n",
    "\n",
    "        if top_k is not None and len(candidates) > top_k:\n",
    "            candidates.sort(key=lambda x: x[2], reverse=True)\n",
    "            candidates = candidates[:top_k]\n",
    "\n",
    "        for f2, dt, m2 in candidates:\n",
    "            h = make_hash(f1, f2, dt)\n",
    "            if h is not None:\n",
    "                out.append((h, t1))\n",
    "\n",
    "    return out\n"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "id": "64bf83ff",
   "metadata": {},
   "source": [
    "## Build hash database (inverted index)\n",
    "\n",
    "We build and store, for each target zone:\n",
    "\n",
    "- `index[zone_name][hash] = [posting, posting, ...]`\n",
    "\n",
    "where `posting = pack_posting(track_id, t_anchor)`.\n",
    "\n",
    "We also store metadata required by the team for matching later (Fs/N/H, peak params, zone params, file list).\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "4744e044",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T14:04:20.531232Z",
     "start_time": "2026-01-16T14:04:20.515685Z"
    }
   },
   "source": [
    "def list_audio_files(folder):\n",
    "    exts = ('*.mp3', '*.wav', '*.m4a')\n",
    "    files = []\n",
    "    for ext in exts:\n",
    "        files.extend(glob.glob(os.path.join(folder, ext)))\n",
    "    files = sorted(files)\n",
    "    return files\n",
    "\n",
    "\n",
    "def build_hash_database(db_audio_path, output_db_file, zones,\n",
    "                        Fs=22050, N=2048, H=1024, bin_max=None,\n",
    "                        peak_dist_freq=12, peak_dist_time=6, peak_thresh=0.01,\n",
    "                        duration=30.0, top_k_targets=5):\n",
    "    \"\"\"Build a hash DB with multiple target zones.\n",
    "\n",
    "    Returns:\n",
    "        db (dict): database structure\n",
    "        report_rows (list[dict]): per-zone stats\n",
    "    \"\"\"\n",
    "\n",
    "    db_files = list_audio_files(db_audio_path)\n",
    "    if not db_files:\n",
    "        raise FileNotFoundError(f\"No audio files found in: {db_audio_path}\")\n",
    "\n",
    "    # index per zone: zone_name -> dict(hash -> list(postings))\n",
    "    index_by_zone = {z[\"name\"]: defaultdict(list) for z in zones}\n",
    "\n",
    "    # stats\n",
    "    per_zone_hash_counts = {z[\"name\"]: 0 for z in zones}\n",
    "    per_track_hash_counts = {z[\"name\"]: [] for z in zones}\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    tracks = {}  # track_id -> filename\n",
    "\n",
    "    for track_id, fpath in enumerate(tqdm(db_files, desc=\"Indexing DB (Task 1)\")):\n",
    "        fname = os.path.basename(fpath)\n",
    "        tracks[track_id] = fname\n",
    "\n",
    "        Y = compute_spectrogram(\n",
    "            fpath, Fs=Fs, N=N, H=H,\n",
    "            duration=duration,\n",
    "            bin_max=bin_max\n",
    "        )\n",
    "\n",
    "        Cmap = compute_constellation_map(\n",
    "            Y,\n",
    "            dist_freq=peak_dist_freq,\n",
    "            dist_time=peak_dist_time,\n",
    "            thresh=peak_thresh\n",
    "        )\n",
    "\n",
    "        peaks = extract_peaks(Y, Cmap)\n",
    "\n",
    "        # generate and store hashes for each zone\n",
    "        for z in zones:\n",
    "            zname = z[\"name\"]\n",
    "            pairs = generate_hashes_from_peaks(peaks, z, top_k=top_k_targets)\n",
    "\n",
    "            for h, t_anchor in pairs:\n",
    "                index_by_zone[zname][h].append(pack_posting(track_id, t_anchor))\n",
    "\n",
    "            per_zone_hash_counts[zname] += len(pairs)\n",
    "            per_track_hash_counts[zname].append(len(pairs))\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "\n",
    "    db = {\n",
    "        \"meta\": {\n",
    "            \"Fs\": Fs, \"N\": N, \"H\": H,\n",
    "            \"duration\": duration,\n",
    "            \"bin_max\": bin_max,\n",
    "            \"peak_params\": {\n",
    "                \"dist_freq\": peak_dist_freq,\n",
    "                \"dist_time\": peak_dist_time,\n",
    "                \"thresh\": peak_thresh\n",
    "            },\n",
    "            \"zones\": zones,\n",
    "            \"top_k_targets\": top_k_targets,\n",
    "            \"created_at_unix\": time.time(),\n",
    "            \"elapsed_seconds\": elapsed\n",
    "        },\n",
    "        \"tracks\": tracks,\n",
    "        \"index\": {zname: dict(index_by_zone[zname]) for zname in index_by_zone}\n",
    "    }\n",
    "\n",
    "    with open(output_db_file, 'wb') as f:\n",
    "        pickle.dump(db, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # Build report rows\n",
    "    report_rows = []\n",
    "    for z in zones:\n",
    "        zname = z[\"name\"]\n",
    "        total_hashes = per_zone_hash_counts[zname]\n",
    "        avg_hashes = float(np.mean(per_track_hash_counts[zname])) if per_track_hash_counts[zname] else 0.0\n",
    "        report_rows.append({\n",
    "            \"zone\": zname,\n",
    "            \"df_bins\": z[\"df\"],\n",
    "            \"dt_min_frames\": z[\"dt_min\"],\n",
    "            \"dt_max_frames\": z[\"dt_max\"],\n",
    "            \"total_hashes\": int(total_hashes),\n",
    "            \"avg_hashes_per_track\": avg_hashes,\n",
    "            \"num_tracks\": len(db_files),\n",
    "            \"top_k_targets\": top_k_targets,\n",
    "        })\n",
    "\n",
    "    return db, report_rows\n"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "id": "3a79cf7f",
   "metadata": {},
   "source": [
    "## Run Task 1: build + save DB\n",
    "\n",
    "This cell will:\n",
    "\n",
    "- build the hash DB\n",
    "- save `hash_db.pkl` into `OUTPUT_DIR`\n",
    "- write `hash_db_report.csv`\n",
    "- show a small summary\n",
    "\n",
    "If `hash_db.pkl` already exists, it will **load** it instead of rebuilding (delete the file if you want to rebuild).\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a8926fc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T14:05:25.423232Z",
     "start_time": "2026-01-16T14:04:24.894321Z"
    }
   },
   "source": [
    "if os.path.exists(OUTPUT_DB_FILE):\n",
    "    print(f\"Found existing DB at: {OUTPUT_DB_FILE}\")\n",
    "    print(\"Loading it...\")\n",
    "    with open(OUTPUT_DB_FILE, 'rb') as f:\n",
    "        db = pickle.load(f)\n",
    "\n",
    "\n",
    "    rows = []\n",
    "    for z in db[\"meta\"][\"zones\"]:\n",
    "        zname = z[\"name\"]\n",
    "        idx = db[\"index\"][zname]\n",
    "        total_hashes = sum(len(v) for v in idx.values())\n",
    "        rows.append({\n",
    "            \"zone\": zname,\n",
    "            \"df_bins\": z[\"df\"],\n",
    "            \"dt_min_frames\": z[\"dt_min\"],\n",
    "            \"dt_max_frames\": z[\"dt_max\"],\n",
    "            \"total_hashes\": int(total_hashes),\n",
    "            \"num_tracks\": len(db[\"tracks\"]),\n",
    "            \"top_k_targets\": db[\"meta\"].get(\"top_k_targets\"),\n",
    "        })\n",
    "\n",
    "    report_df = pd.DataFrame(rows)\n",
    "else:\n",
    "    db, rows = build_hash_database(\n",
    "        DB_AUDIO_PATH,\n",
    "        OUTPUT_DB_FILE,\n",
    "        zones=TARGET_ZONES,\n",
    "        Fs=Fs, N=N, H=H,\n",
    "        bin_max=BIN_MAX,\n",
    "        peak_dist_freq=PEAK_DIST_FREQ,\n",
    "        peak_dist_time=PEAK_DIST_TIME,\n",
    "        peak_thresh=PEAK_THRESH,\n",
    "        duration=LOAD_DURATION,\n",
    "        top_k_targets=TOP_K_TARGETS\n",
    "    )\n",
    "\n",
    "    report_df = pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "file_size_bytes = os.path.getsize(OUTPUT_DB_FILE)\n",
    "report_df[\"db_file_size_bytes\"] = file_size_bytes\n",
    "report_df.to_csv(OUTPUT_REPORT_CSV, index=False)\n",
    "\n",
    "print(\"Task 1 Summary\")\n",
    "print(\"Saved:\", OUTPUT_DB_FILE)\n",
    "print(\"DB size:\", file_size_bytes, \"bytes\")\n",
    "print(\"Saved report:\", OUTPUT_REPORT_CSV)\n",
    "\n",
    "display(report_df)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Indexing DB (Task 1):   0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "Indexing DB (Task 1):   5%|▌         | 1/20 [00:53<16:53, 53.34s/it]\u001B[A\n",
      "Indexing DB (Task 1):  10%|█         | 2/20 [00:54<06:49, 22.78s/it]\u001B[A\n",
      "Indexing DB (Task 1):  15%|█▌        | 3/20 [00:55<03:36, 12.72s/it]\u001B[A\n",
      "Indexing DB (Task 1):  20%|██        | 4/20 [00:55<02:04,  7.80s/it]\u001B[A\n",
      "Indexing DB (Task 1):  25%|██▌       | 5/20 [00:56<01:16,  5.09s/it]\u001B[A\n",
      "Indexing DB (Task 1):  30%|███       | 6/20 [00:56<00:49,  3.51s/it]\u001B[A\n",
      "Indexing DB (Task 1):  35%|███▌      | 7/20 [00:56<00:31,  2.45s/it]\u001B[A\n",
      "Indexing DB (Task 1):  40%|████      | 8/20 [00:57<00:21,  1.76s/it]\u001B[A\n",
      "Indexing DB (Task 1):  45%|████▌     | 9/20 [00:57<00:14,  1.28s/it]\u001B[A\n",
      "Indexing DB (Task 1):  50%|█████     | 10/20 [00:57<00:09,  1.01it/s]\u001B[A\n",
      "Indexing DB (Task 1):  55%|█████▌    | 11/20 [00:57<00:06,  1.32it/s]\u001B[A\n",
      "Indexing DB (Task 1):  60%|██████    | 12/20 [00:58<00:04,  1.66it/s]\u001B[A\n",
      "Indexing DB (Task 1):  65%|██████▌   | 13/20 [00:58<00:03,  1.89it/s]\u001B[A\n",
      "Indexing DB (Task 1):  70%|███████   | 14/20 [00:58<00:02,  2.31it/s]\u001B[A\n",
      "Indexing DB (Task 1):  75%|███████▌  | 15/20 [00:58<00:01,  2.74it/s]\u001B[A\n",
      "Indexing DB (Task 1):  80%|████████  | 16/20 [00:59<00:01,  3.05it/s]\u001B[A\n",
      "Indexing DB (Task 1):  85%|████████▌ | 17/20 [00:59<00:01,  2.84it/s]\u001B[A\n",
      "Indexing DB (Task 1):  90%|█████████ | 18/20 [00:59<00:00,  2.71it/s]\u001B[A\n",
      "Indexing DB (Task 1):  95%|█████████▌| 19/20 [01:00<00:00,  3.19it/s]\u001B[A\n",
      "Indexing DB (Task 1): 100%|██████████| 20/20 [01:00<00:00,  3.01s/it]\u001B[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 Summary\n",
      "Saved: C:/Development/MusicIR\\task1_hash_db\\hash_db.pkl\n",
      "DB size: 7619377 bytes\n",
      "Saved report: C:/Development/MusicIR\\task1_hash_db\\hash_db_report.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "            zone  df_bins  dt_min_frames  dt_max_frames  total_hashes  \\\n",
       "0  Fsmall_Tshort       50              1             20        134524   \n",
       "1   Fsmall_Tlong       50              1             60        160971   \n",
       "2  Flarge_Tshort      150              1             20        161834   \n",
       "3   Flarge_Tlong      150              1             60        163209   \n",
       "\n",
       "   avg_hashes_per_track  num_tracks  top_k_targets  db_file_size_bytes  \n",
       "0               6726.20          20              5             7619377  \n",
       "1               8048.55          20              5             7619377  \n",
       "2               8091.70          20              5             7619377  \n",
       "3               8160.45          20              5             7619377  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zone</th>\n",
       "      <th>df_bins</th>\n",
       "      <th>dt_min_frames</th>\n",
       "      <th>dt_max_frames</th>\n",
       "      <th>total_hashes</th>\n",
       "      <th>avg_hashes_per_track</th>\n",
       "      <th>num_tracks</th>\n",
       "      <th>top_k_targets</th>\n",
       "      <th>db_file_size_bytes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fsmall_Tshort</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>134524</td>\n",
       "      <td>6726.20</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>7619377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fsmall_Tlong</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>160971</td>\n",
       "      <td>8048.55</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>7619377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Flarge_Tshort</td>\n",
       "      <td>150</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>161834</td>\n",
       "      <td>8091.70</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>7619377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Flarge_Tlong</td>\n",
       "      <td>150</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>163209</td>\n",
       "      <td>8160.45</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>7619377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "id": "7cf26ef3",
   "metadata": {},
   "source": [
    "## Statistics\n",
    "\n",
    "- number of tracks\n",
    "- number of unique hashes per zone\n",
    "- length of some posting lists\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "2c6bf60e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T14:05:45.184152Z",
     "start_time": "2026-01-16T14:05:45.147404Z"
    }
   },
   "source": [
    "print(\"Tracks:\", len(db[\"tracks\"]))\n",
    "\n",
    "for z in db[\"meta\"][\"zones\"]:\n",
    "    zname = z[\"name\"]\n",
    "    idx = db[\"index\"][zname]\n",
    "    unique_hashes = len(idx)\n",
    "    total_postings = sum(len(v) for v in idx.values())\n",
    "    avg_postings_per_hash = total_postings / unique_hashes if unique_hashes else 0\n",
    "\n",
    "    print(f\"Zone: {zname}\")\n",
    "    print(\"  unique hashes:\", unique_hashes)\n",
    "    print(\"  total postings:\", total_postings)\n",
    "    print(\"  avg postings/hash:\", avg_postings_per_hash)\n",
    "\n",
    "    # show a few example posting-list lengths\n",
    "    if unique_hashes:\n",
    "        sample_keys = list(idx.keys())[:5]\n",
    "        print(\"  sample posting list lengths:\", [len(idx[k]) for k in sample_keys])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracks: 20\n",
      "Zone: Fsmall_Tshort\n",
      "  unique hashes: 105660\n",
      "  total postings: 134524\n",
      "  avg postings/hash: 1.2731781184932804\n",
      "  sample posting list lengths: [2, 1, 1, 1, 1]\n",
      "Zone: Fsmall_Tlong\n",
      "  unique hashes: 133589\n",
      "  total postings: 160971\n",
      "  avg postings/hash: 1.2049719662547067\n",
      "  sample posting list lengths: [1, 1, 1, 1, 1]\n",
      "Zone: Flarge_Tshort\n",
      "  unique hashes: 130838\n",
      "  total postings: 161834\n",
      "  avg postings/hash: 1.2369036518442653\n",
      "  sample posting list lengths: [1, 1, 1, 1, 1]\n",
      "Zone: Flarge_Tlong\n",
      "  unique hashes: 136604\n",
      "  total postings: 163209\n",
      "  avg postings/hash: 1.194760036309332\n",
      "  sample posting list lengths: [1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "execution_count": 33
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
